{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from common.actor_critic import ActorCritic\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from common.minipacman import MiniPacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>USE CUDA</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pixels and Rewards</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Outputs of the Environment Model is trained to predict the next frame and reward by stochastic gradient decent on the Bernoulli cross-entropy between network outputs and data.<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 different pixels in MiniPacman\n",
    "pixels = (\n",
    "    (0.0, 1.0, 1.0),\n",
    "    (0.0, 1.0, 0.0), \n",
    "    (0.0, 0.0, 1.0),\n",
    "    (1.0, 1.0, 1.0),\n",
    "    (1.0, 1.0, 0.0), \n",
    "    (0.0, 0.0, 0.0),\n",
    "    (1.0, 0.0, 0.0),\n",
    ")\n",
    "pixel_to_categorical = {pix:i for i, pix in enumerate(pixels)} \n",
    "num_pixels = len(pixels)\n",
    "\n",
    "#For each mode in MiniPacman there are different rewards\n",
    "mode_rewards = {\n",
    "    \"regular\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"avoid\":   [0.1, -0.1, -5, -10, -20],\n",
    "    \"hunt\":    [0, 1, 10, -20],\n",
    "    \"ambush\":  [0, -0.1, 10, -20],\n",
    "    \"rush\":    [0, -0.1, 9.9]\n",
    "}\n",
    "reward_to_categorical = {mode: {reward:i for i, reward in enumerate(mode_rewards[mode])} for mode in mode_rewards.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pix_to_target(next_states):\n",
    "    target = []\n",
    "    for pixel in next_states.transpose(0, 2, 3, 1).reshape(-1, 3):\n",
    "        target.append(pixel_to_categorical[tuple([np.ceil(pixel[0]), np.ceil(pixel[1]), np.ceil(pixel[2])])])\n",
    "    return target\n",
    "\n",
    "def target_to_pix(imagined_states):\n",
    "    pixels = []\n",
    "    to_pixel = {value: key for key, value in pixel_to_categorical.items()}\n",
    "    for target in imagined_states:\n",
    "        pixels.append(list(to_pixel[target]))\n",
    "    return np.array(pixels)\n",
    "\n",
    "def rewards_to_target(mode, rewards):\n",
    "    target = []\n",
    "    for reward in rewards:\n",
    "        target.append(reward_to_categorical[mode][reward])\n",
    "    return target\n",
    "\n",
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('loss %s' % losses[-1])\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    \n",
    "def displayImage(image, step, reward):\n",
    "    s = str(step) + \" \" + str(reward)\n",
    "    plt.title(s)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Environment Model</h2>\n",
    "<p>The input and output frames are of size 3 x 15 x 19 (RGB x width x height). The model consisted of a size preserving, multi-scale CNN architecture with additional fully connected layers for reward prediction. <br>In order to capture long-range dependencies across pixels, we also make use of a layer called pool-and-inject, which applies global max-pooling over each\n",
    "feature map and broadcasts the resulting values as feature maps of the same size and concatenates the\n",
    "result to the input. Pool-and-inject layers are therefore size-preserving layers which communicate the\n",
    "max-value of each layer globally to the next convolutional layer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_shape, n1, n2, n3):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.n3 = n3\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=in_shape[1:])\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0] * 2, n1, kernel_size=1, stride=2, padding=6),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n1, n1, kernel_size=10, stride=1, padding=(5, 6)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0] * 2, n2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n2, n2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(n1 + n2,  n3, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.pool_and_inject(inputs)\n",
    "        x = torch.cat([self.conv1(x), self.conv2(x)], 1)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat([x, inputs], 1)\n",
    "        return x\n",
    "    \n",
    "    def pool_and_inject(self, x):\n",
    "        pooled     = self.maxpool(x)\n",
    "        tiled      = pooled.expand((x.size(0),) + self.in_shape)\n",
    "        out        = torch.cat([tiled, x], 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvModel(nn.Module):\n",
    "    def __init__(self, in_shape, num_pixels, num_rewards):\n",
    "        super(EnvModel, self).__init__()\n",
    "        \n",
    "        width  = in_shape[1]\n",
    "        height = in_shape[2]\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(8, 64, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.basic_block1 = BasicBlock((64, width, height), 16, 32, 64)\n",
    "        self.basic_block2 = BasicBlock((128, width, height), 16, 32, 64)\n",
    "        \n",
    "        self.image_conv = nn.Sequential(\n",
    "            nn.Conv2d(192, 256, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.image_fc = nn.Linear(256, num_pixels)\n",
    "        \n",
    "        self.reward_conv = nn.Sequential(\n",
    "            nn.Conv2d(192, 64, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.reward_fc    = nn.Linear(64 * width * height, num_rewards)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.basic_block1(x)\n",
    "        x = self.basic_block2(x)\n",
    "        \n",
    "        image = self.image_conv(x)\n",
    "        image = image.permute(0, 2, 3, 1).contiguous().view(-1, 256)\n",
    "        image = self.image_fc(image)\n",
    "\n",
    "        reward = self.reward_conv(x)\n",
    "        reward = reward.view(batch_size, -1)\n",
    "        reward = self.reward_fc(reward)\n",
    "        \n",
    "        return image, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating environments</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"regular\"\n",
    "num_envs = 16\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = MiniPacman(mode, 1000)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape\n",
    "num_actions = envs.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model    = EnvModel(envs.observation_space.shape, num_pixels, len(mode_rewards[\"regular\"]))\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(env_model.parameters())\n",
    "\n",
    "if USE_CUDA:\n",
    "    env_model    = env_model.cuda()\n",
    "    actor_critic = actor_critic.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading pretrained Actor Critic from previous notebook.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic.load_state_dict(torch.load(\"actor_critic_\" + mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    if state.ndim == 4:\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "    else:\n",
    "        state = torch.FloatTensor(np.float32(state)).unsqueeze(0)\n",
    "        \n",
    "    action = actor_critic.act(Variable(state, volatile=True))\n",
    "    action = action.data.cpu().squeeze(1).numpy()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(envs, frames):\n",
    "    states = envs.reset()\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        actions = get_action(states)\n",
    "        next_states, rewards, dones, _ = envs.step(actions)\n",
    "        \n",
    "        yield frame_idx, states, actions, rewards, next_states, dones\n",
    "        \n",
    "        states = next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_coef = 0.1\n",
    "num_updates = 5000\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "\n",
    "for frame_idx, states, actions, rewards, next_states, dones in play_games(envs, num_updates):\n",
    "    states      = torch.FloatTensor(states)\n",
    "    actions     = torch.LongTensor(actions)\n",
    "\n",
    "    batch_size = states.size(0)\n",
    "    \n",
    "    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
    "    onehot_actions[range(batch_size), actions] = 1\n",
    "    inputs = Variable(torch.cat([states, onehot_actions], 1))\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    imagined_state, imagined_reward = env_model(inputs)\n",
    "\n",
    "    target_state = pix_to_target(next_states)\n",
    "    target_state = Variable(torch.LongTensor(target_state))\n",
    "    \n",
    "    target_reward = rewards_to_target(mode, rewards)\n",
    "    target_reward = Variable(torch.LongTensor(target_reward))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    image_loss  = criterion(imagined_state, target_state)\n",
    "    reward_loss = criterion(imagined_reward, target_reward)\n",
    "    loss = image_loss + reward_coef * reward_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.data[0])\n",
    "    all_rewards.append(np.mean(rewards))\n",
    "    \n",
    "    if frame_idx % 10 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Saving the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(env_model.state_dict(), \"env_model_\" + mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imagination!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAACwCAYAAAAys3i6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDfbi4JL48gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe010053bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = MiniPacman(mode, 1000)\n",
    "batch_size = 1\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "iss = []\n",
    "ss  = []\n",
    "\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    steps += 1\n",
    "    actions = get_action(state)\n",
    "    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
    "    onehot_actions[range(batch_size), actions] = 1\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    \n",
    "    inputs = Variable(torch.cat([state, onehot_actions], 1))\n",
    "    if USE_CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    imagined_state, imagined_reward = env_model(inputs)\n",
    "    imagined_state = F.softmax(imagined_state)\n",
    "    iss.append(imagined_state)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(actions[0])\n",
    "    ss.append(state)\n",
    "    state = next_state\n",
    "    \n",
    "    imagined_image = target_to_pix(imagined_state.view(batch_size, -1, len(pixels))[0].max(1)[1].data.cpu().numpy())\n",
    "    imagined_image = imagined_image.reshape(15, 19, 3)\n",
    "    state_image = torch.FloatTensor(next_state).permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    clear_output()\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Imagined\")\n",
    "    plt.imshow(imagined_image)\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Actual\")\n",
    "    plt.imshow(state_image)\n",
    "    plt.show()\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    if steps > 30:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
